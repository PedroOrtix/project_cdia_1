# AnÃ¡lisis Comparativo de ResNet: Impacto del NÃºmero de Bloques en el Aprendizaje

## ğŸ“‹ DescripciÃ³n
Este proyecto investiga cÃ³mo el nÃºmero de bloques residuales en una arquitectura ResNet afecta su capacidad de aprendizaje y rendimiento. Se realizan experimentos comparativos con diferentes configuraciones de ResNet para analizar la relaciÃ³n entre la profundidad de la red y su efectividad.

## ğŸ¯ Objetivos
- Comparar el rendimiento de ResNet con diferentes nÃºmeros de bloques residuales
- Analizar la velocidad de convergencia en el entrenamiento
- Evaluar la precisiÃ³n final alcanzada por cada configuraciÃ³n
- Identificar la relaciÃ³n Ã³ptima entre profundidad y rendimiento

## ğŸ› ï¸ Requisitos
```python
pytorch-lightning>=2.0.0
torch>=2.0.0
torchvision>=0.15.0
datasets>=2.0.0
transformers>=4.0.0
tensorboard>=2.0.0
pillow>=8.0.0
torchmetrics>=0.11.0
```

## ğŸ“ Estructura del Proyecto
```
project/
â”œâ”€â”€ data/                    # Directorio para datasets
â”œâ”€â”€ lightning_modules/       # MÃ³dulos de PyTorch Lightning
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ restnet_module.py    # ImplementaciÃ³n de ResNet
â”œâ”€â”€ models/                  # Modelos guardados
â”‚   â””â”€â”€ checkpoints/        # Checkpoints durante el entrenamiento
â”œâ”€â”€ notebooks/              # Jupyter notebooks para anÃ¡lisis
â”œâ”€â”€ results/                # Resultados y mÃ©tricas
â”œâ”€â”€ scripts/                # Scripts de entrenamiento
â”‚   â”œâ”€â”€ fine_tuning_function.py
â”‚   â””â”€â”€ train_animals.py
â”œâ”€â”€ utils/                  # Utilidades
â”‚   â””â”€â”€ data_loader.py      # Funciones de carga de datos
â””â”€â”€ requirements.txt        # Dependencias del proyecto
```

## ğŸš€ CaracterÃ­sticas Principales

### ResNet Transfer Learning
- Modelo base: ResNet18 pre-entrenado en ImageNet
- Capacidad de congelar bloques selectivamente para transfer learning
- MÃ©tricas implementadas:
  - Accuracy (entrenamiento, validaciÃ³n y prueba)
  - PÃ©rdida (entrenamiento, validaciÃ³n y prueba)
- OptimizaciÃ³n:
  - Optimizador: AdamW
  - Learning Rate Scheduler: ReduceLROnPlateau
  - PrecisiÃ³n mixta para optimizaciÃ³n de memoria

### Dataset
- Dataset: Animals de Hugging Face
- 90 clases diferentes de animales
- DivisiÃ³n automÃ¡tica en conjuntos de entrenamiento y validaciÃ³n
- Transformaciones de datos:
  - Redimensionamiento a 224x224
  - NormalizaciÃ³n con medias y desviaciones estÃ¡ndar de ImageNet

## ğŸ’» Uso

### InstalaciÃ³n
```bash
# Clonar el repositorio
git clone [URL_DEL_REPOSITORIO]
cd project

# Instalar dependencias
pip install -r requirements.txt
```

### Entrenamiento
```bash
# Entrenar el modelo con configuraciÃ³n por defecto
python scripts/train_animals.py

# Los parÃ¡metros configurables incluyen:
- num_classes: NÃºmero de clases (default: 90)
- batch_size: TamaÃ±o del batch (default: 32)
- learning_rate: Tasa de aprendizaje (default: 3e-5)
- max_epochs: NÃºmero mÃ¡ximo de Ã©pocas (default: 30)
- freeze_blocks: NÃºmero de bloques a congelar (default: 3)
```

### Monitoreo
```bash
# Visualizar mÃ©tricas en TensorBoard
tensorboard --logdir results/animals/logs
```

## ğŸ“Š CaracterÃ­sticas del Entrenamiento

### ConfiguraciÃ³n del Modelo
- **Arquitectura Base**: ResNet18
- **Transfer Learning**: 
  - Pesos pre-entrenados de ImageNet
  - Capacidad de congelar hasta 4 bloques
  - Capa final adaptada a 90 clases

### OptimizaciÃ³n
- **Optimizador**: AdamW
- **Learning Rate**: 3e-5 (configurable)
- **Scheduler**: ReduceLROnPlateau
  - Factor de reducciÃ³n: 0.5
  - Paciencia: 3 Ã©pocas
  - Monitoreo: val_loss

### Callbacks
- **Model Checkpoint**: Guarda los mejores modelos basados en val_loss
- **Early Stopping**: Detiene el entrenamiento si no hay mejora
- **TensorBoard Logger**: Registra mÃ©tricas de entrenamiento

## ğŸ“ˆ MÃ©tricas y Logging
- **MÃ©tricas de Entrenamiento**:
  - Loss (por paso y Ã©poca)
  - Accuracy (por paso y Ã©poca)
- **MÃ©tricas de ValidaciÃ³n**:
  - Loss
  - Accuracy
- **VisualizaciÃ³n**: TensorBoard para seguimiento en tiempo real

## ğŸ¤ Contribuciones
Las contribuciones son bienvenidas. Por favor, sigue estos pasos:
1. Fork el repositorio
2. Crea una rama para tu feature
3. Commit tus cambios
4. Push a la rama
5. Abre un Pull Request

## ğŸ“ Licencia
Este proyecto estÃ¡ bajo la licencia [ESPECIFICAR_LICENCIA].