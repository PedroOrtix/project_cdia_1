# Optimizar Clasificaci√≥n de Beans mediante Congelamiento Selectivo en ResNet18

## Descripci√≥n / Abstract

Este estudio desaf√≠a la creencia com√∫n de que entrenar todos los par√°metros de un modelo preentrenado conduce necesariamente a mejores resultados. Utilizando una √∫nica arquitectura ResNet18, investigamos c√≥mo el congelamiento estrat√©gico de diferentes bloques puede mantener o incluso mejorar el rendimiento mientras se reduce significativamente el n√∫mero de par√°metros entrenables y los recursos computacionales necesarios. El objetivo es demostrar que una configuraci√≥n m√°s eficiente de los par√°metros entrenables puede lograr resultados comparables o superiores, optimizando as√≠ el uso de recursos computacionales sin comprometer el rendimiento.

---

## Introducci√≥n

Si bien hist√≥ricamente existi√≥ una tendencia a utilizar todos los par√°metros disponibles en modelos preentrenados, actualmnete es bien conocida la importancia cr√≠tica de la eficiencia computacional. Este cambio de paradigma est√° impulsado por la necesidad de democratizar el acceso a los modelos de aprendizaje profundo, haci√©ndolos m√°s accesibles y pr√°cticos para su implementaci√≥n en servicios del mundo real. 

Esta tendencia hacia la democratizaci√≥n se refleja en el desarrollo de diversas t√©cnicas de optimizaci√≥n como:
- **LoRA** (Low-Rank Adaptation), que permite el fine-tuning eficiente mediante la factorizaci√≥n de matrices
- **Destilaci√≥n de conocimiento** para crear modelos m√°s compactos
- **Cuantizaci√≥n de pesos** para reducir los requisitos de memoria y acelerar la inferencia

En este contexto, el estudio utiliza la ResNet18 como caso de estudio para demostrar que, mediante el congelamiento selectivo de capas, podemos reducir significativamente la cantidad de par√°metros entrenables mientras mantenemos o mejoramos el rendimiento del modelo.

La optimizaci√≥n de recursos sin comprometer el rendimiento se ha convertido en un factor crucial para hacer que las soluciones de inteligencia artificial sean verdaderamente escalables y accesibles para todo el mundo, desde peque√±as empresas hasta aplicaciones de gran escala.

---

## Metodolog√≠a/Modelo

### Dataset üìä

El estudio utiliza el dataset "Beans" proporcionado por AI-Lab-Makerere a trav√©s de HuggingFace, que consiste en im√°genes de plantas de frijol para clasificaci√≥n. Los datos se procesan mediante un pipeline de transformaciones que incluye:

- üîÑ Redimensionamiento de im√°genes a 224x224 p√≠xeles
- üìä Normalizaci√≥n utilizando los valores est√°ndar de ImageNet:
  - mean=[0.485, 0.456, 0.406]
  - std=[0.229, 0.224, 0.225]
- üî¢ Transformaci√≥n a tensores para su procesamiento en PyTorch

El dataset se divide en tres conjuntos: entrenamiento, validaci√≥n y test, manteniendo la estructura original proporcionada por los autores.

### Arquitectura del Modelo üèóÔ∏è

Se utiliza una ResNet18 pre-entrenada en ImageNet (IMAGENET1K_v1) como modelo base, modificada para la tarea de clasificaci√≥n espec√≠fica:

- **Modelo base**: ResNet18 con pesos pre-entrenados
- **Capa de clasificaci√≥n**: Adaptada para 3 clases (correspondientes a las categor√≠as de plantas de frijol)
- **Estrategia de congelamiento**: Implementaci√≥n de congelamiento selectivo por bloques, permitiendo experimentar con diferentes configuraciones de par√°metros entrenables

### Proceso de Entrenamiento ‚öôÔ∏è

#### Configuraci√≥n Base:
| Par√°metro | Valor |
|-----------|--------|
| Batch size | 16 |
| Learning rate | 1e-4 |
| Max epochs | 30 |
| Workers | 4 |

#### Elementos de la estrategia de training:
- ‚úÖ Callback para guardar checkpoints con filtro en val_loss con pacience = 10
- üõë Early stopping con filtro en val_loss con pacience = 10
- üìä Logger para TensorBoard
- ‚öôÔ∏è Declaracacion del trainer con:
  - Precisi√≥n mixta para optimizar memoria y velocidad
  - Compatibilidad con GPU (Nvidia T4 de Google Colab)
  - deterministic=True para asegurar reproducibilidad
  - log_every_n_steps=10 para registrar m√©tricas cada 10 steps

#### Decisiones de dise√±o:
- üìà Uso de tensorboard para registrar m√©tricas y visualizaciones
- ‚ö° Uso de pytorch-lightning para agilizar el desarrollo
- üéØ Epochs limitadas a 30 con early stopping para prevenir overfitting

---

## Resultados
### üìä An√°lisis Experimental de Resultados

#### 1. Rendimiento del Modelo üéØ

##### 1.1 Accuracy de Entrenamiento y Validaci√≥n
![Accuracies de Entrenamiento y Validaci√≥n](./results/plots/accuracies.png)

> Puesto que se trata de un problema de clasificaci√≥n sencillo, hasta la congelaci√≥n de la mayor√≠a del modelo rinde excepcionalmente bien. Esto se ha hecho para remarcar a√∫n m√°s el hecho de que no necesitamos hacer full-finetunings ni grandes entrenamientos para obtener resultados notables. Incluso se puede ver que a partir de la congelaci√≥n de los dos √∫ltimos bloques el rendimiento extra es residual.

##### 1.2 Evoluci√≥n del Loss Durante el Entrenamiento
![Loss durante el Entrenamineto](./results/plots/loss_progress.png)

> La visualizaci√≥n del loss durante el entrenamiento es bastante ilustrativa. Puesto que se trata de un problema de clasificaci√≥n sencilla, el modelo converge r√°pidamente. Adem√°s, se puede ver c√≥mo el loss durante la validaci√≥n es bastante estable y se mantiene parejo al del training, d√°ndonos a entender que no estamos sufriendo de overfitting.

#### 2. An√°lisis de Generalizaci√≥n üîç

##### 2.1 Estudio de Overfitting
![Estudio de Overfitting](./results/plots/overfitting_analysis.png)

> La gr√°fica de an√°lisis de overfitting muestra claramente que no hay un overfitting latente en el modelo. El que m√°s indicios de generalizaci√≥n muestra es el modelo de 4 bloques congelados puesto que tiene que lidiar con todo un "forward pass" de 4 bloques de ResNet18 sin ning√∫n par√°metro entrenable.

##### 2.2 An√°lisis de Convergencia
![Comparativa de Convergencia](./results/plots/convergence_analysis.png)

> Como es natural, cuanto m√°s par√°metros m√°s r√°pido va a converger el modelo. Le va a costar muy poco encontrar un √≥ptimo, todo lo contrario al modelo de 4 bloques congelados cuya convergencia es m√°s lenta. La desviaci√≥n est√°ndar del 4 es m√°s alta, sus steps son m√°s abruptos.

#### 3. Eficiencia Computacional ‚ö°

##### 3.1 Relaci√≥n Par√°metros-Tiempo
![Parametros vs Tiempo de Entrenamiento](./results/plots/params_vs_time.png)

> Esto ejemplifica muy bien c√≥mo el experimento al ser peque√±o, se da la dicotom√≠a entre los tiempos similares de entrenamiento y el n√∫mero de par√°metros entrenables.
> Puesto que el cuello de botella no est√° en el training, sino en el batch size y los dataloaders. Esto hace que usar una GPU para problemas peque√±os no sea una buena inversi√≥n de recursos.

#### 4. Resultados Finales üéØ

##### 4.1 Rendimiento en Test
![Accuracy en Test](./results/plots/test_results.png)

> Es de esperar que el modelo de 4 bloques congelados tenga el menor accuracy puesto que es el que menos par√°metros entrenables tiene. Adem√°s, se puede ver c√≥mo el modelo de 1 bloque congelado rinde casi igual que el de 2 y 3, nos da a pensar que no compensa usar m√°s par√°metros entrenables, solo los necesarios.

## Conclusiones

Los resultados obtenidos en este estudio respaldan firmemente nuestra hip√≥tesis inicial sobre la optimizaci√≥n de recursos mediante el congelamiento selectivo de capas en arquitecturas preentrenadas. A trav√©s de una serie de experimentos sistem√°ticos con ResNet18, hemos demostrado que no es necesario entrenar todos los par√°metros del modelo para obtener un rendimiento √≥ptimo en tareas de clasificaci√≥n espec√≠ficas.

El an√°lisis detallado de diferentes configuraciones de congelamiento revel√≥ que podemos mantener un accuracy bueno para este caso de uso (superior al 95%) incluso cuando congelamos hasta tres bloques del modelo. Este hallazgo es particularmente significativo desde la perspectiva de la eficiencia computacional y la optimizaci√≥n de recursos. La degradaci√≥n significativa del rendimiento solo se observ√≥ al congelar cuatro bloques, lo que sugiere un punto de equilibrio claro entre la reducci√≥n de par√°metros entrenables y el mantenimiento del rendimiento.

Un aspecto especialmente relevante del experimento es la estabilidad del entrenamiento y la ausencia de overfitting en todas las configuraciones probadas. Los modelos mostraron una convergencia saludable y una generalizaci√≥n robusta, como se evidencia en las m√©tricas de validaci√≥n y test. Esto sugiere que el congelamiento selectivo no solo optimiza recursos, sino que tambi√©n puede contribuir a una mejor generalizaci√≥n del modelo.

La relaci√≥n entre el tiempo de entrenamiento y el n√∫mero de par√°metros revel√≥ una perspectiva interesante sobre la eficiencia computacional en problemas de escala moderada. Observamos que, para datasets relativamente peque√±os, el cuello de botella principal no reside en el proceso de entrenamiento sino en la gesti√≥n de datos, lo que cuestiona la necesidad de recursos computacionales intensivos como GPUs para problemas similares.

Estos resultados tienen implicaciones significativas para la democratizaci√≥n del aprendizaje profundo, aline√°ndose con la tendencia actual hacia t√©cnicas de optimizaci√≥n como LoRA y la destilaci√≥n de conocimiento. Nuestro enfoque demuestra que es posible implementar soluciones eficientes y efectivas sin necesidad de recursos computacionales extensivos, haciendo que las aplicaciones de deep learning sean m√°s accesibles para un espectro m√°s amplio de usuarios y casos de uso.


