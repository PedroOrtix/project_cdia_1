# OptimizaciÃ³n de Recursos mediante Congelamiento Selectivo en ResNet18

## DescripciÃ³n / Abstract

Este estudio desafÃ­a la creencia comÃºn de que entrenar todos los parÃ¡metros de un modelo preentrenado conduce necesariamente a mejores resultados. Utilizando una Ãºnica arquitectura ResNet18, investigamos cÃ³mo el congelamiento estratÃ©gico de diferentes bloques puede mantener o incluso mejorar el rendimiento mientras se reduce significativamente el nÃºmero de parÃ¡metros entrenables y los recursos computacionales necesarios. El objetivo es demostrar que una configuraciÃ³n mÃ¡s eficiente de los parÃ¡metros entrenables puede lograr resultados comparables o superiores, optimizando asÃ­ el uso de recursos computacionales sin comprometer el rendimiento.

---

## IntroducciÃ³n

Si bien histÃ³ricamente existiÃ³ una tendencia a utilizar todos los parÃ¡metros disponibles en modelos preentrenados, actualmnete es bien conocida la importancia crÃ­tica de la eficiencia computacional. Este cambio de paradigma estÃ¡ impulsado por la necesidad de democratizar el acceso a los modelos de aprendizaje profundo, haciÃ©ndolos mÃ¡s accesibles y prÃ¡cticos para su implementaciÃ³n en servicios del mundo real. 

Esta tendencia hacia la democratizaciÃ³n se refleja en el desarrollo de diversas tÃ©cnicas de optimizaciÃ³n como:
- **LoRA** (Low-Rank Adaptation), que permite el fine-tuning eficiente mediante la factorizaciÃ³n de matrices
- **DestilaciÃ³n de conocimiento** para crear modelos mÃ¡s compactos
- **CuantizaciÃ³n de pesos** para reducir los requisitos de memoria y acelerar la inferencia

En este contexto, nuestro estudio utiliza la ResNet18 como caso de estudio para demostrar que, mediante el congelamiento selectivo de capas, podemos reducir significativamente la cantidad de parÃ¡metros entrenables mientras mantenemos o mejoramos el rendimiento del modelo.

La optimizaciÃ³n de recursos sin comprometer el rendimiento se ha convertido en un factor crucial para hacer que las soluciones de inteligencia artificial sean verdaderamente escalables y accesibles para todo el mundo, desde pequeÃ±as empresas hasta aplicaciones de gran escala.

---

## MetodologÃ­a/Modelo

### Dataset ğŸ“Š

El estudio utiliza el dataset "Beans" proporcionado por AI-Lab-Makerere a travÃ©s de HuggingFace, que consiste en imÃ¡genes de plantas de frijol para clasificaciÃ³n. Los datos se procesan mediante una pipeline de transformaciones que incluye:

- ğŸ”„ Redimensionamiento de imÃ¡genes a 224x224 pÃ­xeles
- ğŸ“Š NormalizaciÃ³n utilizando los valores estÃ¡ndar de ImageNet:
  - mean=[0.485, 0.456, 0.406]
  - std=[0.229, 0.224, 0.225]
- ğŸ”¢ TransformaciÃ³n a tensores para su procesamiento en PyTorch

El dataset se divide en tres conjuntos: entrenamiento, validaciÃ³n y test, manteniendo la estructura original proporcionada por los autores.

### Arquitectura del Modelo ğŸ—ï¸

Se utiliza una ResNet18 pre-entrenada en ImageNet (IMAGENET1K_v1) como modelo base, modificada para la tarea de clasificaciÃ³n especÃ­fica:

- **Modelo base**: ResNet18 con pesos pre-entrenados
- **Capa de clasificaciÃ³n**: Adaptada para 3 clases (correspondientes a las categorÃ­as de plantas de frijol)
- **Estrategia de congelamiento**: ImplementaciÃ³n de congelamiento selectivo por bloques, permitiendo experimentar con diferentes configuraciones de parÃ¡metros entrenables

### Proceso de Entrenamiento âš™ï¸

#### ConfiguraciÃ³n Base:
| ParÃ¡metro | Valor |
|-----------|--------|
| Batch size | 16 |
| Learning rate | 1e-4 |
| Max epochs | 30 |
| Workers | 4 |

#### Elementos de la estrategia de training:
- âœ… Callback para guardar checkpoints con filtro en val_loss con pacience = 10
- ğŸ›‘ Early stopping con filtro en val_loss con pacience = 10
- ğŸ“Š Logger para TensorBoard
- âš™ï¸ Declaracacion del trainer con:
  - PrecisiÃ³n mixta para optimizar memoria y velocidad
  - Compatibilidad con GPU (Nvidia T4 de Google Colab)
  - deterministic=True para asegurar reproducibilidad
  - log_every_n_steps=10 para registrar mÃ©tricas cada 10 steps

#### Decisiones de diseÃ±o:
- ğŸ“ˆ Uso de tensorboard para registrar mÃ©tricas y visualizaciones
- âš¡ Uso de pytorch-lightning para agilizar el desarrollo
- ğŸ¯ Epochs limitadas a 30 con early stopping para prevenir overfitting

---

## Resultados
### AnÃ¡lisis de Resultados ğŸ“Š

#### Accuracy de Entrenamiento y ValidaciÃ³n ğŸ“ˆ

![Accuracies de Entrenamiento y ValidaciÃ³n](./results/beans/accuracies.png)




## Conclusiones

[Las conclusiones detalladas se aÃ±adirÃ¡n una vez se hayan analizado todos los resultados experimentales, enfocÃ¡ndose en la relaciÃ³n entre parÃ¡metros entrenables y rendimiento]
